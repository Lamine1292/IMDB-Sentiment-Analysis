{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Exploratory Data Analysis for IMDB Movie Reviews**",
   "id": "43fafc79b25ea6e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploratory Data Analysis for IMDB Movie Reviews\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Load dataset\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")"
   ],
   "id": "a5ba051d404ee22d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Overview\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.head(5))\n",
    "print(\"\\nMissing values per column:\\n\", df.isnull().sum())\n",
    "print(\"Duplicate rows:\", df.duplicated().sum())"
   ],
   "id": "9fbfa9bfa067619e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Simple cleaning for text analysis\n",
    "def simple_clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'<.*?>', ' ', text)          # Remove HTML tags\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' ', text) # Remove URLs\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)       # Keep letters only\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "df['clean_review'] = df['review'].apply(simple_clean)"
   ],
   "id": "787a11e48872e94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Stopwords and tokenization\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['word_count'] = df['clean_review'].apply(lambda x: len(x.split()))\n",
    "print(\"\\nWord count stats:\\n\", df['word_count'].describe())"
   ],
   "id": "fb288c5aae76ec5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 5. Class distribution\n",
    "print(\"\\nSentiment distribution:\\n\", df['sentiment'].value_counts())"
   ],
   "id": "5e3e903f879bfaa2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 6. Top tokens overall\n",
    "all_tokens = \" \".join(df['clean_review']).split()\n",
    "filtered = [t for t in all_tokens if t not in stop_words and len(t)>1]\n",
    "top20 = Counter(filtered).most_common(10)\n",
    "print(\"\\nTop 10 most frequent tokens (excluding stopwords):\\n\", top20)"
   ],
   "id": "98bc526f36968e2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 7. Top tokens by sentiment\n",
    "top_pos = Counter([t for t in \" \".join(df[df['sentiment']=='positive']['clean_review']).split() if t not in stop_words and len(t)>1]).most_common(10)\n",
    "top_neg = Counter([t for t in \" \".join(df[df['sentiment']=='negative']['clean_review']).split() if t not in stop_words and len(t)>1]).most_common(10)\n",
    "print(\"\\nTop positive tokens:\\n\", top_pos)\n",
    "print(\"\\nTop negative tokens:\\n\", top_neg)"
   ],
   "id": "cd6735cf362dc1d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 8. Visualization\n",
    "plt.figure(figsize=(6,4))\n",
    "counts = df['sentiment'].value_counts()\n",
    "plt.bar(counts.index, counts.values, color=['skyblue','salmon'])\n",
    "plt.title(\"Class Distribution (Positive vs Negative)\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(df['word_count'], bins=50, color='purple', alpha=0.7)\n",
    "plt.title(\"Distribution of Review Lengths (Word Count)\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim(0, 1000)\n",
    "plt.show()"
   ],
   "id": "c9ab8b0f0b9c7004",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Positive WordCloud\n",
    "pos_text = \" \".join(df[df['sentiment']=='positive']['clean_review'])\n",
    "wc_pos = WordCloud(width=800, height=400, background_color='white').generate(pos_text)\n",
    "plt.imshow(wc_pos, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Positive Reviews WordCloud\")\n",
    "plt.show()\n",
    "\n",
    "# Negative WordCloud\n",
    "neg_text = \" \".join(df[df['sentiment']=='negative']['clean_review'])\n",
    "wc_neg = WordCloud(width=800, height=400, background_color='white').generate(neg_text)\n",
    "plt.imshow(wc_neg, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Negative Reviews WordCloud\")\n",
    "plt.show()"
   ],
   "id": "3ac076c842cf62dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 9. Save preview of cleaned dataset\n",
    "df[['clean_review','sentiment','word_count']].head(200).to_csv(\"IMDB_preview_clean.csv\", index=False)\n",
    "print(\"\\nâœ… Preview saved as: IMDB_preview_clean.csv\")"
   ],
   "id": "a1fb840e6e259176",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b4741ea17c275016",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================\n",
    "# Baseline Models for IMDB\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# ------------------------\n",
    "# 1. Load IMDB dataset\n",
    "# ------------------------\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "# ------------------------\n",
    "# 2. Cleaning\n",
    "# ------------------------\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z ]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_review\"] = df[\"review\"].apply(clean_text)\n",
    "\n",
    "# ------------------------\n",
    "# 3. Train-test split\n",
    "# ------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"clean_review\"], df[\"sentiment\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ------------------------\n",
    "# 4. TF-IDF Vectorization\n",
    "# ------------------------\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# ------------------------\n",
    "# 5. Logistic Regression\n",
    "# ------------------------\n",
    "lr = LogisticRegression(max_iter=200)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "lr_pred = lr.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\n====================\")\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"====================\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, lr_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, lr_pred, average=\"weighted\"))\n",
    "print(confusion_matrix(y_test, lr_pred))\n",
    "\n",
    "# ------------------------\n",
    "# 6. Naive Bayes\n",
    "# ------------------------\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "nb_pred = nb.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\n====================\")\n",
    "print(\"NAIVE BAYES\")\n",
    "print(\"====================\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, nb_pred))\n",
    "print(\"F1-score:\", f1_score(y_test, nb_pred, average=\"weighted\"))\n",
    "print(confusion_matrix(y_test, nb_pred))\n"
   ],
   "id": "91d18d56dc3668b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install transformers datasets torch --quiet",
   "id": "98f0580ece59af35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ============================\n",
    "# BERT Fine-Tuning on IMDB\n",
    "# ============================\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load IMDB dataset\n",
    "# ----------------------------\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Tokenization\n",
    "# ----------------------------\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Load BERT model\n",
    "# ----------------------------\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Training arguments\n",
    "# ----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-imdb\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Trainer\n",
    "# ----------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"]\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "#                                    6. Train model\n",
    "# ----------------------------\n",
    "trainer.train()\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Evaluate\n",
    "# ----------------------------\n",
    "results = trainer.evaluate()\n",
    "print(\"\\n===== BERT RESULTS =====\")\n",
    "print(results)\n"
   ],
   "id": "4a9db2a566f4a5d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "f5f31621ec2d5694",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = load_dataset(\"imdb\")",
   "id": "55d44ad927617a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")",
   "id": "d342ecd0a976df1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ],
   "id": "ad3f077958d29e39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)",
   "id": "ab5499ca51cedd74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-imdb\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200,\n",
    ")"
   ],
   "id": "d7c2c290ab7d29dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-imdb\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200,)"
   ],
   "id": "fd16be5925af01ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pip install --upgrade transformers",
   "id": "2c44559177d082f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-imdb\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200,)"
   ],
   "id": "fc546475fb397124",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-imdb\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500\n",
    ")"
   ],
   "id": "47c97b4308a9138",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-imdb\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500)\n"
   ],
   "id": "abc9f644c7b95c38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-imdb\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500)\n"
   ],
   "id": "6a39d01395f014e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pip install --upgrade transformers accelerate datasets",
   "id": "4bb7b4d6790b5c7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3cd8cb65330b9d40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from transformers import TrainingArguments, Trainer",
   "id": "afa90ee8d87b05c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "training_args = TrainingArguments(...)",
   "id": "1c0c20417e8bdf33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = Trainer(...)\n",
    "trainer.train()"
   ],
   "id": "4e60048b6ca96cbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_imdb_output\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200)"
   ],
   "id": "b380bebc42324295",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "30ef6608d067b161",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Load BERT model for binary classification (positive/negative)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2\n",
    ")"
   ],
   "id": "c23a2060b8113657",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset"
   ],
   "id": "3e6b1d0ab4743b85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset = load_dataset(\"imdb\")",
   "id": "b2a2de5f94327909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")",
   "id": "8f523e557057c172",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)"
   ],
   "id": "3bab32ce69205810",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = dataset.map(tokenize, batched=True)\n",
    "dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ],
   "id": "17e228e7729295e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2)"
   ],
   "id": "abf473423b67539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_imdb_output\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200)"
   ],
   "id": "869b395f8ccc93a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"])"
   ],
   "id": "92ef0b8466756585",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "trainer.train() stop",
   "id": "d8027db53db15ead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "61c369d0af9a6ebe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
